plugins {
  id 'nf-azure'
}

py3Image = "quay.io/h3abionet_org/py3plink"
gemmaImage="quay.io/h3abionet_org/py3plink"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
py2fastImage="quay.io/h3abionet_org/py2fastlmm"

swarmPort = '2376'
queue = 'batch'

manifest {
    homePage = 'http://github.com/h3abionet/h3agwas'
    description = 'GWAS Pipeline for H3Africa'
    mainScript = "main.nf"
}


aws {
    accessKey =''
    secretKey =''
    region    ='us-east-1'
}

/*cloud {
    // imageId = "ami-710b9108"      // specify your AMI id here
    instanceType = "m4.xlarge"
    subnetId = "null"
    sharedStorageId   = "null"
    sharedStorageMount = "/mnt/shared"
    bootStorageSize = "20GB"     // Size of disk for images spawned
    //   instanceStorageMount = ""   // Set a common mount point for images
    //   instanceStorageDevice = ""  // Set a common block device for images
    autoscale {
        enabled = true
        maxInstances = 1
        terminateWhenIdle = true
    }

}
*/


params {

    // Directories
    work_dir                = "/$PWD"
    input_dir               = "sample"
    // Can use S3 too
    //input_dir               = "s3://h3abionet/sample"
    input_pat               = "sampleA"

    output_dir              = "${params.work_dir}/output"
    scripts                 = "${params.work_dir}/scripts"
    output                  = "out"

    max_forks            = 95     

    high_ld_regions_fname = ""
    sexinfo_available    = true
    cut_het_high         = 0.343
    cut_het_low           = 0.15
    cut_diff_miss         = "0.05"
    cut_maf                = "0.01"
    cut_mind              = "0.02"
    cut_geno              = 0.01
    cut_hwe               = 0.008
    pi_hat                = 0.11
    super_pi_hat 	  = 0.7
    f_lo_male             = 0.8 // default for F-sex check -- >= means male
    f_hi_female           = 0.2 //  <= means female
    gc10                 = 0.4  // 10% Gen Call confidence -- 0.4 is very low
    case_control         = "${params.input_dir}/sample.phe"
    case_control_col     = "PHE"
    build_genome = "hg19"

    phenotype = "0"
    pheno_col = ""
    batch = "0"
    batch_col = 0

    idpat                =  0  // or "(\\w+)-DNA_(\\w+)_.*" or ".*_(.*)"

    plink_mem_req        = "1750MB"
    other_mem_req        = "1750MB"
    big_time             = '12h'
    sharedStorageMount   = "/mnt/shared"
    max_plink_cores      = 4
    autosome_plink = "--chr 1-22,25" 
cut_maf_xfemale =0.01
cut_maf_xmale = 0.01
cut_miss_xmale = 0.02
cut_miss_xfemale = 0.02
cut_diffmiss_x = 0.02



}



profiles {


    // For execution on a local machine, no containerization. -- Default
    standard {
        process.executor = 'local'
    }

    awsbatch {
         process.executor = "awsbatch"
	 aws.region    ='us-east-1'
         aws.uploadStorageClass = 'ONEZONE_IA'
         process.queue = 'h3a-00'
    }

    azurebatch {
        process.executor = 'azurebatch'
    }

    slurm {
       process.executor = 'slurm'
       process.queue = queue
     }



    // Execute pipeline with Docker locally
    docker {
        docker.remove      = true
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
	docker.runOptions = '-u $(id -u):$(id -g) --rm'
        docker.process.executor = 'local'
    }


    // Execute pipeline with Docker Swarm setup
    dockerSwarm {
        docker.remove      = true
        docker.runOptions  = '--rm'
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
        docker.process.executor = 'local'
        docker.engineOptions = "-H :$swarmPort"
    }

    // For execution on a PBS scheduler, no containerization.
    pbs {
        process.executor = 'pbs'
        process.queue = queue
    }




    // For execution on a SLURM scheduler, no containerization.
    slurm {
        process.executor = 'slurm'
        process.queue = queue
    }


    slurmSingularity {
        singularity.cacheDir = "${HOME}/.singularity"
        process.executor = 'slurm'
        singularity.autoMounts = true
        singularity.enabled = true
        singularity.runOption = "--cleanenv"
        process.queue = queue

    }





    singularity {
        singularity.cacheDir = "${HOME}/.singularity"
        singularity.autoMounts = true
        singularity.enabled = true
     }
}






process {

    withLabel:bigMem {
      memory = '8GB'
    }

    container=py3Image

    withLabel: latex {
          container = latexImage
    }

    withLabel: py2fast {
          container = py2fastImage 
    }

}

timeline { 
    enabled=true
    file = "nextflow_reports/timeline.html"
}

report {
    enabled = true
    file = "nextflow_reports/report.html"
}
