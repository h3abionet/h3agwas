plugins {
  id 'nf-azure'
}

py3Image = "quay.io/h3abionet_org/py3plink"
gemmaImage="quay.io/h3abionet_org/py3plink"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
//py2fastImage="quay.io/h3abionet_org/py2fastlmm"
Py3Utils="quay.io/h3abionet_org/py2fastlmm"
finemappingImage="quay.io/h3abionet_org/py3finemapping"
gemmaImage="quay.io/h3abionet_org/py3plink"
boltImage="quay.io/h3abionet_org/py3fastlmm"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
py3saige="quay.io/h3abionet_org/py3saige"
gctaImage="quay.io/h3abionet_org/gcta"
MetaAnImage="quay.io/h3abionet_org/py3metagwas"
fastlmmImage="quay.io/h3abionet_org/py3fastlmm"
rImage="quay.io/h3abionet_org/py3rproject"


nextflow.enable.moduleBinaries = true


swarmPort = '2376'
queue = 'batch'

manifest {
    homePage = 'http://github.com/h3abionet/h3agwas'
    description = 'GWAS Pipeline for H3Africa'
    mainScript = "main.nf"
}


aws {
    accessKey =''
    secretKey =''
    region    ='us-east-1'
}

params {
    // Directories
    work_dir                = "/$PWD"
    input_dir               = "sample"
    // Can use S3 too
    //input_dir               = "s3://h3abionet/sample"
    input_pat               = "sampleA"
    bfile = ""

    output_dir              = "${params.work_dir}/output"
    scripts                 = "${params.work_dir}/scripts"
    output                  = "out"

    max_forks            = 95     

    high_ld_regions_fname = ""
    sexinfo_available    = true
    cut_het_high         = 0.343
    cut_het_low           = 0.15
    cut_diff_miss         = "0.05"
    cut_maf                = "0.01"
    cut_mind              = "0.02"
    cut_geno              = 0.01
    cut_hwe               = 0.008
    pi_hat                = 0.11
    super_pi_hat 	  = 0.7
    f_lo_male             = 0.8 // default for F-sex check -- >= means male
    f_hi_female           = 0.2 //  <= means female
    gc10                 = 0.4  // 10% Gen Call confidence -- 0.4 is very low
    case_control         = "${params.input_dir}/sample.phe"
    case_control_col     = "PHE"
    build_genome = "hg19"

    phenotype = "0"
    pheno_col = ""
    batch = "0"
    batch_col = 0

    idpat                =  0  // or "(\\w+)-DNA_(\\w+)_.*" or ".*_(.*)"

    plink_mem_req        = "1750MB"
    low_memory = "2GB"
    big_time             = '12h'
    sharedStorageMount   = "/mnt/shared"
    max_plink_cores      = 4
    autosome_plink = "--chr 1-22,25" 
    chrxx_plink = "23" 
    chrxy_plink = "25" 
    chry_plink = "24"
    cut_maf_xfemale =0.01
    cut_maf_xmale = 0.01
    cut_maf_y = 0.01
    cut_miss_y = 0.02
    cut_miss_xmale = 0.02
    cut_miss_xfemale = 0.02
    cut_diffmiss_x = 0.02
    remove_on_bp = 1

    action = ""

    samplesheet = ""

    //quality control 
    qc = 0
   //
   qc_michigan = 0
   bin_checkmich="HRC-1000G-check-bim.pl"
   dataref_michigan = 
   ftp_dataref_michigan="ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz"
   dataref_michigan=""

}



profiles {
    // For execution on a local machine, no containerization. -- Default
    standard {
        process.executor = 'local'
    }

    awsbatch {
         process.executor = "awsbatch"
	 aws.region    ='us-east-1'
         aws.uploadStorageClass = 'ONEZONE_IA'
         process.queue = 'h3a-00'
    }

    azurebatch {
        process.executor = 'azurebatch'
    }

    slurm {
       process.executor = 'slurm'
       process.queue = queue
     }

    // Execute pipeline with Docker locally
    docker {
        docker.remove      = true
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
	docker.runOptions = '-u $(id -u):$(id -g) --rm'
        docker.process.executor = 'local'
    }


    // Execute pipeline with Docker Swarm setup
    dockerSwarm {
        docker.remove      = true
        docker.runOptions  = '--rm'
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
        docker.process.executor = 'local'
        docker.engineOptions = "-H :$swarmPort"
    }

    // For execution on a PBS scheduler, no containerization.
    pbs {
        process.executor = 'pbs'
        process.queue = queue
    }

    // For execution on a SLURM scheduler, no containerization.
    slurm {
        process.executor = 'slurm'
        process.queue = queue
    }


    slurmSingularity {
        singularity.cacheDir = "${HOME}/.singularity"
        process.executor = 'slurm'
        singularity.autoMounts = true
        singularity.enabled = true
        singularity.runOption = "--cleanenv"
        process.queue = queue

    }





    singularity {
        singularity.cacheDir = "${HOME}/.singularity"
        singularity.autoMounts = true
        singularity.enabled = true
     }
}







process {

    withLabel:bigMem {
      memory = '8GB'
    }

    container=py3Image

    withLabel: latex {
          container = latexImage
    }

    withLabel : py3utils{
          container = Py3Utils
    }
    withLabel : saige{
          container = py3saige
    }

    withLabel: finemapping{
          container = finemappingImage
    }

    withLabel: gemma {
          container = gemmaImage
    }

    withLabel: latex {
          container = latexImage
    }
    withLabel: gcta{
          container = gctaImage
    }
    withLabel: metaanalyse {
          container = MetaAnImage
    }
//    withLabel: py2fast {
//          container = py2fastImage
//    }
    withLabel: bolt {
          container = boltImage
    }
    withLabel: R{
          container = rImage
    }

    withLabel : saige{
          container = py3saige
    }

    withLabel: finemapping{
          container = finemappingImage
    }

}
/*
timeline { 
    enabled=true
    file = "nextflow_reports/timeline.html"
}

report {
    enabled = true
    file = "nextflow_reports/report.html"
}*/
