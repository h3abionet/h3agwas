plugins {
  id 'nf-azure'
}

py3Image = "quay.io/h3abionet_org/py3plink"
gemmaImage="quay.io/h3abionet_org/py3plink"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
//py2fastImage="quay.io/h3abionet_org/py2fastlmm"
Py3Utils="quay.io/h3abionet_org/py3utils"
finemappingImage="quay.io/h3abionet_org/py3finemapping"
gemmaImage="quay.io/h3abionet_org/py3plink"
boltImage="quay.io/h3abionet_org/py3fastlmm"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
py3saige="quay.io/h3abionet_org/py3saige"
gctaImage="quay.io/h3abionet_org/gcta"
MetaAnImage="quay.io/h3abionet_org/py3metagwas"
fastlmmImage="quay.io/h3abionet_org/py3fastlmm"
rImage="quay.io/h3abionet_org/py3rproject"
picardImage="broadinstitute/picard"
mtagImage="quay.io/h3abionet_org/py2mtag"
ldscImage="quay.io/h3abionet_org/py2ldsc"



nextflow.enable.moduleBinaries = true


swarmPort = '2376'
queue = 'batch'

manifest {
    homePage = 'http://github.com/h3abionet/h3agwas'
    description = 'GWAS Pipeline for H3Africa'
    mainScript = "main.nf"
}


aws {
    accessKey =''
    secretKey =''
    region    ='us-east-1'
}

params {
    // Directories
    work_dir                = "/$PWD"
    input_dir               = "./"
    // Can use S3 too
    //input_dir               = "s3://h3abionet/sample"
    input_pat               = "sampleA"
    bfile = ""

    output_dir              = "${params.work_dir}/output"
    scripts                 = "${params.work_dir}/scripts"
    output                  = "out"

    max_forks            = 95

    high_ld_regions_fname = ""
    sexinfo_available    = true
    cut_het_high         = 0.343
    cut_het_low           = 0.15
    cut_diff_miss         = "0.05"
    cut_maf                = 0.01
    cut_mind              = 0.02
    cut_geno              = 0.01
    cut_hwe               = 0.008
    pi_hat_dup            = 0.7
    pi_hat                = 0.7
    super_pi_hat 	  = 0.8
    f_lo_male             = 0.8 // default for F-sex check -- >= means male
    f_hi_female           = 0.2 //  <= means female
    gc10                 = 0.4  // 10% Gen Call confidence -- 0.4 is very low
    //case_control         = "${params.input_dir}/sample.phe"
    //case_control_col     = "PHE"
    /* phenotype description */
    /*QC*/
    case_control         = "" // will become deleted in next version
    pheno_col = ""
    case_control_col     = ""
    batch = ""
    help = false
    
   
    /**/
    pheno = ""
    pheno_type ='0'

    covariates =""
    covariates_type =""


    build_genome = "37"
    build_genome_convert = ""
     
    batch_col = 0

    idpat                =  0  // or "(\\w+)-DNA_(\\w+)_.*" or ".*_(.*)"

    plink_mem_req        = "1750MB"
    low_memory = "2GB"
    high_memory = "20GB"
    big_time             = '12h'
    sharedStorageMount   = "/mnt/shared"
    max_cpus = 4
    autosome_plink = "--chr 1-22,25"
    chrxx_plink = "23"
    chrxy_plink = "25"
    chry_plink = "24"
    cut_maf_xfemale =0.01
    cut_maf_xmale = 0.01
    cut_maf_y = 0.01
    cut_miss_y = 0.02
    cut_miss_xmale = 0.02
    cut_miss_xfemale = 0.02
    cut_diffmiss_x = 0.02
    remove_on_bp = 1

    action = ""

    samplesheet = ""

    //quality control
    qc = 0
    qc_dup = 0
    qc_topbottom = 0
    //apply qc michigan
    qc_michigan = 0
    // convert in vcf
    // convert vcf in other build
    vcf_convertbetwen_build = 0
    vcf_lowmem_convert = 0
    // split vcf by chro
   vcf_split_chro = 0
   bin_checkmich="HRC-1000G-check-bim.pl"
   ftp_dataref_michigan="ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1-1/HRC.r1-1.GRCh37.wgs.mac5.sites.tab.gz"
   dataref_michigan=""
   data = ""

   phenores_tr_fct  = ""
   pheno_tr_fct = ""
   pheno_residuals = 0
   add_pcs = 0
   
   genetic_maps = ""
   //
   convertinvcf =0
   convertvcfinplink =0
   convertvcfinbimbam=0
   convertvcf_stat =1
   convertinvcf_parralchro = 0
   file_ref_gzip = ""
   bin_bcftools = "bcftools"
   bin_picard = '/usr/picard/picard.jar'

   convertinvcf_justagtc = 0
   rsinfovcf_deleted_notref = "T"
   rsinfovcf_poshead_chro_inforef=0
   rsinfovcf_poshead_bp_inforef=1
   rsinfovcf_poshead_rs_inforef=2
   rsinfovcf_poshead_a1_inforef=3
   rsinfovcf_poshead_a2_inforef=4
   tmpdir = "tmp/"


   // ftp link
   fasta = ""
   fasta_convert = ""
   ftp_fasta_19 = "https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz"
   ftp_fasta_37 = "http://ftp.ensembl.org/pub/grch37/current/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna.primary_assembly.fa.gz"
   ftp_fasta_38 = "http://ftp.ensembl.org/pub/grch37/current/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna.primary_assembly.fa.gz"
   ftp_fasta = ''
   ftp_fasta_convert = ''
   picard_max_record = 5000
    //pheno add pcs

  //
  impute=0
  vcf = ''
  vcf_list = ''
  vcf_folder_zip_password=''
  vcf_folder_zip=''
  /*automatic detection for software?*/
  vcf_imputeformat= 'minmac4' 
  //vcf_patstatfreq="%AN %AC"
  vcf_patstatfreq=""
  impute_info_cutoff = 0.3 //vcf_minscoreimp=0.3
  //vcf_patscoreimp="INFO/R2"
  vcf_patscoreimp=""
  vcf_cut_miss = -1
  vcf_geno_type="TYPED"
  vcf_convert_version = 4.2
  low_mem_vcf = 0

  vcf_convert_id = "id-paste=iid"

  /*clean plink rel*/
  snps_include_rel = ""
  snps_exclude_rel = ""
  snps_justacgt_rel = 1
  snps_thincount_rel = ""
  cut_maf_rel = 0.01
  plink_indep_pairwise='100 20 0.1'

 /*association test*/
  association = 0
  saige = 0
  regenie = 0
  assoc = 0
  fisher = 0
  logistic  = 0
  model = 0
  cmh = 0
  linear = 0
  
  /**/
  saige_bin_fitmodel="step1_fitNULLGLMM.R"                                 
  saige_bin_spatest="step2_SPAtests.R"                                     
  saige_otheropt_step1 =''
  saige_otheropt_step2 =''
  saige_otheropt  =''
 saige_impute_method="best_guess"
/*regenie option*/
  regenie_bsize=100
  regenie_bsize_step1=0
  regenie_bsize_step2=0
  regenie_otheropt_step1 = ""
  regenie_otheropt_step2 = ""
  regenie_otheropt = ""
  regenie_bin="regenie"
/*plink assoc perm*/
  mperm = 0 
  adjust = 0
  /*imputation*/
  eagle_genetic_map = ""
  ftp_eagle_genetic_map = ""
  ftp_eagle_genetic_map_38 = "https://storage.googleapis.com/broad-alkesgroup-public/Eagle/downloads/tables/genetic_map_hg38_withX.txt.gz"
  ftp_eagle_genetic_map_19 = "https://storage.googleapis.com/broad-alkesgroup-public/Eagle/downloads/tables/genetic_map_hg19_withX.txt.gz"
  /*list_chro*/
  imp_listchro= 'ALL'
  imp_ref_name = "default"
  imp_ref_vcf= ""
  imp_ref_m3vcf = ""
  imp_min_ac = 2
  imp_min_alleles= 2
  imp_max_alleles= 2
  imp_chunk_size  = "20000000" // in base
  imp_chunk = ''
  imp_buffer_size = "1000000"
  imp_minRatio = 0.01
  /*option assoc*/
  loco= 1
  gxe = ''
  imputed_data = 1
  /*for association */
  vcf_field="DS"                                                           
  vcf_minmac=5   
 //bgen 
 bgen=""
 bgen_list = ''
 bgen_sample =''
 


  mtag = 0
  // sumstat
  file_gwas = ""
  dir_ref_ld_chr =""
  sumstat_head_chr =""
  sumstat_head_bp = ""
  sumstat_head_a1 =""
  sumstat_head_a2 =""
  sumstat_head_pval = ""
  sumstat_head_rs =""
  sumstat_head_freq =""
  sumstat_head_z =""
  sumstat_head_beta =""
  sumstat_head_se =""
  sumstat_head_n =""
  sumstat_list_n = ""
  munge_sumstats_bin="munge_sumstats.py"
  munge_keep = ""
  col_fidid="FID,IID"
  newcol_fidid="IID_update"

  cut_maf_postimp= 0.00001
  bin_mtag="mtag.py"
  opt_mtag =""
  ldsc = 0
  bin_ldsc="ldsc.py"

ldsc_h2opt =""

}



profiles {
    // For execution on a local machine, no containerization. -- Default
    standard {
        process.executor = 'local'
    }

    awsbatch {
         process.executor = "awsbatch"
	 aws.region    ='us-east-1'
         aws.uploadStorageClass = 'ONEZONE_IA'
         process.queue = 'h3a-00'
    }

    azurebatch {
        process.executor = 'azurebatch'
    }

    slurm {
       process.executor = 'slurm'
       process.queue = queue
     }

    // Execute pipeline with Docker locally
    docker {
        docker.remove      = true
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
	docker.runOptions = '-u $(id -u):$(id -g) --rm'
        docker.process.executor = 'local'
    }


    // Execute pipeline with Docker Swarm setup
    dockerSwarm {
        docker.remove      = true
        docker.runOptions  = '--rm'
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
        docker.process.executor = 'local'
        docker.engineOptions = "-H :$swarmPort"
    }

    // For execution on a PBS scheduler, no containerization.
    pbs {
        process.executor = 'pbs'
        process.queue = queue
    }

    // For execution on a SLURM scheduler, no containerization.
    slurm {
        process.executor = 'slurm'
        process.queue = queue
    }


    slurmSingularity {
        singularity.cacheDir = "${HOME}/.singularity"
        process.executor = 'slurm'
        singularity.autoMounts = true
        singularity.enabled = true
        singularity.runOption = "--cleanenv"
        process.queue = queue

    }





    singularity {
        singularity.cacheDir = "${HOME}/.singularity"
        singularity.autoMounts = true
        singularity.enabled = true
     }
}







process {

    withLabel:bigMem {
      memory = '8GB'
    }

    container=py3Image

    withLabel: latex {
          container = latexImage
    }

    withLabel : py3utils{
          container = Py3Utils
    }
    withLabel : saige{
          container = py3saige
    }

    withLabel: finemapping{
          container = finemappingImage
    }

    withLabel: gemma {
          container = gemmaImage
    }

    withLabel: latex {
          container = latexImage
    }
    withLabel: gcta{
          container = gctaImage
    }
    withLabel: metaanalyse {
          container = MetaAnImage
    }
//    withLabel: py2fast {
//          container = py2fastImage
//    }
    withLabel: bolt {
          container = boltImage
    }
    withLabel: R{
          container = rImage
    }

    withLabel : saige{
          container = py3saige
    }

    withLabel: finemapping{
          container = finemappingImage
    }
    withLabel: picard{
          container = picardImage
    }
    withLabel : mtag{
          container = mtagImage
    }
    withLabel : py2ldsc{
          container = ldscImage
    }
    withLabel : utils{
          container = Py3Utils
    }
   withLabel : imputation {
      container = 'quay.io/h3abionet_org/imputation_tools' // Container slug. Stable releases should specify release tag!
   }

}
/*
timeline {
    enabled=true
    file = "nextflow_reports/timeline.html"
}

report {
    enabled = true
    file = "nextflow_reports/report.html"
}*/
